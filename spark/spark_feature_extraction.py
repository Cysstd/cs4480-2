# spark_feature_extraction_from_pig_fixed.py - ä¿®å¤NumPyé”™è¯¯

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, size
from pyspark.sql.types import ArrayType, DoubleType
import numpy as np
import json

# åˆå§‹åŒ–Spark
spark = SparkSession.builder \
    .appName("FeatureExtractionFromPigFixed") \
    .config("spark.executor.memory", "2g") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()

def extract_features_from_64x64_fixed(image_array_str):
    """ä¿®å¤ç‰ˆç‰¹å¾æå– - ä½¿ç”¨å­˜åœ¨çš„NumPyå‡½æ•°"""
    try:
        # è§£æå›¾åƒæ•°ç»„
        image_array = np.array(json.loads(image_array_str), dtype=np.uint8)
        
        # éªŒè¯å°ºå¯¸
        if len(image_array.shape) != 3:
            print(f"âŒ é3Då›¾åƒï¼Œè·³è¿‡: {image_array.shape}")
            return []
            
        height, width, channels = image_array.shape
        
        # ç¡®ä¿æ˜¯64Ã—64Ã—3
        if height != 64 or width != 64:
            print(f"âš ï¸ éå¸¸è§„å°ºå¯¸ {height}Ã—{width}ï¼Œä½¿ç”¨å½“å‰å°ºå¯¸ç»§ç»­å¤„ç†")
        
        print(f"âœ… å¤„ç†å›¾åƒ: {height}Ã—{width}Ã—{channels}")
        
        features = []
        
        # ==================== é¢œè‰²ç‰¹å¾æå– ====================
        if channels >= 3:
            # ä½¿ç”¨å‰3ä¸ªé€šé“ (RGB)
            for channel in range(3):
                channel_data = image_array[:, :, channel].flatten()
                # è®¡ç®—çœŸå®çš„ç»Ÿè®¡ç‰¹å¾
                features.extend([
                    float(np.mean(channel_data)),          # å‡å€¼
                    float(np.std(channel_data)),           # æ ‡å‡†å·®  
                    float(np.median(channel_data)),        # ä¸­ä½æ•°
                    float(np.percentile(channel_data, 25)), # 25%åˆ†ä½æ•°
                    float(np.percentile(channel_data, 75)), # 75%åˆ†ä½æ•°
                ])
        else:
            # å•é€šé“å›¾åƒï¼Œé‡å¤è®¡ç®—ä¿æŒç‰¹å¾æ•°ä¸€è‡´
            channel_data = image_array[:, :, 0].flatten()
            for _ in range(3):
                features.extend([
                    float(np.mean(channel_data)),
                    float(np.std(channel_data)),
                    float(np.median(channel_data)),
                    float(np.percentile(channel_data, 25)),
                    float(np.percentile(channel_data, 75)),
                ])
        
        # ==================== çº¹ç†ç‰¹å¾æå– ====================
        # è½¬æ¢ä¸ºç°åº¦å›¾
        if channels >= 3:
            gray_img = np.dot(image_array[...,:3], [0.2989, 0.5870, 0.1140])  # æ ‡å‡†RGBè½¬ç°åº¦
        else:
            gray_img = image_array[:, :, 0]
        
        gray_flat = gray_img.flatten()
        
        # åŸºç¡€ç°åº¦ç»Ÿè®¡ - ä½¿ç”¨å­˜åœ¨çš„NumPyå‡½æ•°
        features.extend([
            float(np.mean(gray_flat)),     # ç°åº¦å‡å€¼
            float(np.std(gray_flat)),      # ç°åº¦æ ‡å‡†å·®
            float(np.var(gray_flat)),      # æ–¹å·® (æ›¿ä»£skew)
        ])
        
        # å›¾åƒç†µ (çº¹ç†å¤æ‚åº¦)
        try:
            histogram, _ = np.histogram(gray_img, bins=64, range=(0, 255))
            histogram = histogram.astype(float)
            histogram += 1e-8  # é¿å…log(0)
            histogram /= histogram.sum()
            entropy = -np.sum(histogram * np.log2(histogram))
            features.append(float(entropy))
        except Exception as e:
            print(f"âš ï¸ ç†µè®¡ç®—å¤±è´¥: {e}")
            features.append(0.0)
        
        # ==================== å¯¹æ¯”åº¦ç‰¹å¾ ====================
        try:
            # ä½¿ç”¨RMSå¯¹æ¯”åº¦
            contrast = float(np.std(gray_img))
            features.append(contrast)
        except:
            features.append(0.0)
            
        # ==================== æ–°å¢ç®€å•å½¢çŠ¶ç‰¹å¾ ====================
        try:
            # è®¡ç®—å›¾åƒäº®åº¦åˆ†å¸ƒç‰¹å¾
            brightness_mean = float(np.mean(gray_img))
            brightness_std = float(np.std(gray_img))
            
            # æ·»åŠ æ›´å¤šåŸºç¡€ç»Ÿè®¡ç‰¹å¾
            features.extend([
                float(np.min(gray_img)),    # æœ€å°å€¼
                float(np.max(gray_img)),    # æœ€å¤§å€¼
                float(np.percentile(gray_img, 10)),  # 10%åˆ†ä½æ•°
                float(np.percentile(gray_img, 90)),  # 90%åˆ†ä½æ•°
            ])
        except Exception as e:
            print(f"âš ï¸ å½¢çŠ¶ç‰¹å¾å¤±è´¥: {e}")
            features.extend([0.0] * 4)
            
        print(f"âœ… çœŸå®æå–äº† {len(features)} ä¸ªç‰¹å¾ï¼ŒèŒƒå›´: [{min(features):.3f}, {max(features):.3f}]")
        return features
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–é”™è¯¯: {e}")
        return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œè®©Sparkè¿‡æ»¤

# æ³¨å†ŒUDF
extract_features_udf = udf(extract_features_from_64x64_fixed, ArrayType(DoubleType()))

def validate_no_hardcoding(df):
    """éªŒè¯æ²¡æœ‰ç¡¬ç¼–ç ç‰¹å¾"""
    print("\nğŸ” æ£€æŸ¥ç¡¬ç¼–ç ç‰¹å¾...")
    
    samples = df.select("image_features").limit(10).collect()
    
    hardcoded_count = 0
    for i, row in enumerate(samples):
        features = row.image_features
        if features:
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯0 (ç¡¬ç¼–ç å«Œç–‘)
            if all(abs(f) < 1e-6 for f in features):
                hardcoded_count += 1
                print(f"âŒ æ ·æœ¬ {i}: æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯0 (ç¡¬ç¼–ç å«Œç–‘)")
            else:
                # æ£€æŸ¥ç‰¹å¾å€¼çš„åˆç†æ€§
                non_zero_features = [f for f in features if abs(f) > 1e-6]
                feature_range = max(features) - min(features)
                if feature_range > 1.0 and len(non_zero_features) > 0:
                    print(f"âœ… æ ·æœ¬ {i}: ç‰¹å¾æ­£å¸¸ [{min(features):.3f}, {max(features):.3f}]")
                else:
                    print(f"âš ï¸  æ ·æœ¬ {i}: ç‰¹å¾èŒƒå›´è¿‡å° [{min(features):.3f}, {max(features):.3f}]")
    
    if hardcoded_count > 0:
        print(f"âŒ å‘ç° {hardcoded_count} ä¸ªç¡¬ç¼–ç æ ·æœ¬")
        return False
    else:
        print("âœ… æœªå‘ç°ç¡¬ç¼–ç ç‰¹å¾")
        return True

def main():
    print("=" * 60)
    print("ğŸš€ Sparkç‰¹å¾æå– - ä¿®å¤NumPyé”™è¯¯")
    print("=" * 60)
    
    # ä½¿ç”¨Pigè¾“å‡ºçš„ç¡®åˆ‡è·¯å¾„
    pig_output_path = "hdfs://localhost:9000/user/hadoop/image_analysis/cleaned_data_with_images_json_fixed"
    
    try:
        # è¯»å–Pigæ¸…æ´—åçš„æ•°æ®
        cleaned_data = spark.read.json(pig_output_path)
        record_count = cleaned_data.count()
        print(f"âœ… æˆåŠŸè¯»å–Pigè¾“å‡ºæ•°æ®: {record_count} æ¡è®°å½•")
        
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–Pigè¾“å‡ºæ•°æ®: {e}")
        spark.stop()
        return
    
    # æ•°æ®é¢„è§ˆ
    print("\nğŸ” æ•°æ®é¢„è§ˆ:")
    cleaned_data.select("image_id", "label", "keyword").show(5, truncate=30)
    
    # ==================== ç‰¹å¾æå– ====================
    print("\n" + "=" * 60)
    print("ğŸ”§ å¼€å§‹ç‰¹å¾æå– (ä¿®å¤NumPyé”™è¯¯)...")
    print("=" * 60)
    
    # åº”ç”¨ç‰¹å¾æå–
    features_df = cleaned_data.withColumn("image_features", extract_features_udf(col("image_array")))
    
    # è¿‡æ»¤æ‰æå–å¤±è´¥çš„è®°å½• (è¿”å›ç©ºåˆ—è¡¨çš„)
    valid_features_df = features_df.filter(size("image_features") > 0)
    
    failed_count = features_df.count() - valid_features_df.count()
    print(f"ç‰¹å¾æå–ç»“æœ:")
    print(f"  âœ… æˆåŠŸ: {valid_features_df.count()} æ¡è®°å½•")
    print(f"  âŒ å¤±è´¥: {failed_count} æ¡è®°å½•")
    
    if valid_features_df.count() == 0:
        print("âŒ æ‰€æœ‰ç‰¹å¾æå–éƒ½å¤±è´¥äº†!")
        spark.stop()
        return
    
    # ==================== éªŒè¯ç‰¹å¾è´¨é‡ ====================
    print("\n" + "=" * 60)
    print("ğŸ”¬ éªŒè¯ç‰¹å¾è´¨é‡")
    print("=" * 60)
    
    # æ£€æŸ¥ç¡¬ç¼–ç 
    no_hardcoding = validate_no_hardcoding(valid_features_df)
    
    # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
    print("\nğŸ“Š ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯:")
    feature_samples = valid_features_df.select("image_features").limit(5).collect()
    
    for i, row in enumerate(feature_samples):
        features = row.image_features
        print(f"æ ·æœ¬ {i}:")
        print(f"  ç‰¹å¾æ•°: {len(features)}")
        print(f"  èŒƒå›´: [{min(features):.3f}, {max(features):.3f}]")
        print(f"  å‡å€¼: {np.mean(features):.3f} Â± {np.std(features):.3f}")
        print(f"  éé›¶ç‰¹å¾: {sum(1 for f in features if abs(f) > 1e-6)}/{len(features)}")
    
    # ==================== ä¿å­˜ç»“æœ ====================
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜ç‰¹å¾æ•°æ®...")
    print("=" * 60)
    
    # é€‰æ‹©éœ€è¦çš„åˆ—
    final_df = valid_features_df.select(
        "image_id",
        "label", 
        "keyword",
        "tags", 
        "image_features",
        "original_shape"
    )
    
    # ä¿å­˜åˆ°HDFS
    output_path = "hdfs://localhost:9000/user/hadoop/image_analysis/extracted_features_final"
    
    try:
        final_df.write.mode("overwrite").json(output_path)
        print(f"âœ… ç‰¹å¾æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        
        # éªŒè¯ä¿å­˜
        saved_count = spark.read.json(output_path).count()
        print(f"âœ… éªŒè¯: æˆåŠŸä¿å­˜ {saved_count} æ¡è®°å½•")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    # ==================== è´¨é‡æŠ¥å‘Š ====================
    print("\n" + "=" * 60)
    print("ğŸ“‹ ç‰¹å¾è´¨é‡æŠ¥å‘Š")
    print("=" * 60)
    
    if no_hardcoding:
        print("ğŸ‰ ç‰¹å¾è´¨é‡ä¼˜ç§€!")
        print("   âœ… æ— ç¡¬ç¼–ç ç‰¹å¾")
        print("   âœ… ç‰¹å¾å€¼èŒƒå›´åˆç†")
        print("   âœ… çœŸæ­£ä»å›¾åƒæ•°æ®è®¡ç®—")
    else:
        print("âš ï¸  ç‰¹å¾è´¨é‡æœ‰é—®é¢˜:")
        print("   âŒ å‘ç°ç¡¬ç¼–ç ç‰¹å¾")
    
    print(f"\nâœ¨ ä¿®å¤å®Œæˆ!")
    print(f"åŸå§‹æ•°æ®: {record_count} æ¡")
    print(f"æœ‰æ•ˆç‰¹å¾: {valid_features_df.count()} æ¡")
    print(f"ç‰¹å¾ç»´åº¦: {len(feature_samples[0].image_features) if feature_samples else 'æœªçŸ¥'}")
    
    spark.stop()

if __name__ == "__main__":
    main()