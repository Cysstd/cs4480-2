file://<WORKSPACE>/spark/spark_train.scala
empty definition using pc, found symbol in pc: 
semanticdb not found
empty definition using fallback
non-local guesses:
	 -evaluation.
	 -evaluation#
	 -evaluation().
	 -scala/Predef.evaluation.
	 -scala/Predef.evaluation#
	 -scala/Predef.evaluation().
offset: 361
uri: file://<WORKSPACE>/spark/spark_train.scala
text:
```scala
// spark_train.scala
// Usage:
//   $SPARK_HOME/bin/spark-shell --master local[*]
//   scala> :load spark_train.scala

import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.evaluation@@.BinaryClassificationEvaluator

// ====== CONFIG â€“ EDIT THESE PATHS ======
val featuresPath = "hdfs://fedora:9000/user/hadoop/image_analysis/spark_features"
// =======================================

// Use existing Spark session from spark-shell
println(s"ðŸ“¥ Loading preprocessed features from: $featuresPath")

val df = spark.read.parquet(featuresPath)

// Show schema so you can see what's inside
println("ðŸ“‹ Input schema:")
df.printSchema()

println("ðŸ‘€ Sample rows:")
df.show(5, truncate = false)

// ========= 1. Define LABEL: is_popular =========
// Simple rule: popular if downloads >= 1000
// Adjust this threshold if your data is small/large.
val withLabel = df.withColumn(
  "label",
  when($"downloads" >= 1000, 1.0).otherwise(0.0)
)

// (Optional) check class balance
println("ðŸ“Š Class distribution (label):")
withLabel.groupBy("label").count().show()

// ========= 2. Select feature columns =========

val featureCols = Array(
  "imageWidth",
  "imageHeight",
  "imageSize",
  "views",
  "likes",
  "comments",
  "collections",
  "tag_count",
  "tag_text_len",
  "has_animals",
  "has_people",
  "has_city",
  "is_vector",
  "is_large",
  "aspect_ratio",
  "image_pixels"
)

// Filter out rows with nulls in any of these important columns
val clean = withLabel.na.drop("any", featureCols :+ "label")

println(s"âœ… Rows before cleaning: ${withLabel.count()}")
println(s"âœ… Rows after cleaning:  ${clean.count()}")

// ========= 3. Train-test split =========

val Array(train, test) = clean.randomSplit(Array(0.8, 0.2), seed = 42L)
println(s"ðŸ“š Train size: ${train.count()}, Test size: ${test.count()}")

// ========= 4. ML pipeline: VectorAssembler + LogisticRegression =========

val assembler = new VectorAssembler()
  .setInputCols(featureCols)
  .setOutputCol("features")

val lr = new LogisticRegression()
  .setFeaturesCol("features")
  .setLabelCol("label")
  .setMaxIter(50)
  .setRegParam(0.01)

// Pipeline: features â†’ LR
val pipeline = new Pipeline().setStages(Array(assembler, lr))

println("ðŸš€ Training Logistic Regression model...")
val model: PipelineModel = pipeline.fit(train)

println("âœ… Model trained. Evaluating on test set...")

// ========= 5. Evaluation =========

val predictions = model.transform(test)

println("ðŸ”Ž Predictions (sample):")
predictions.select("image_id", "label", "prediction", "probability").show(10, truncate = false)

// Binary classification evaluator, using area under ROC
val evaluator = new BinaryClassificationEvaluator()
  .setLabelCol("label")
  .setRawPredictionCol("rawPrediction")
  .setMetricName("areaUnderROC")

val auc = evaluator.evaluate(predictions)
println(f"ðŸ“ˆ Test AUC = $auc%1.4f")

// Confusion matrix style counts
println("ðŸ§® Confusion matrix (label vs prediction):")
predictions.groupBy("label", "prediction").count().show()

// ========= 6. (Optional) Save the trained model =========

val modelPath = "hdfs://fedora:9000/user/hadoop/image_analysis/spark_lr_model"
println(s"ðŸ’¾ Saving model to: $modelPath")

model.write.overwrite().save(modelPath)

println("âœ… Training script finished.")

```


#### Short summary: 

empty definition using pc, found symbol in pc: 