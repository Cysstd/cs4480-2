file://<WORKSPACE>/spark/spark_ml_comparison.scala
empty definition using pc, found symbol in pc: 
semanticdb not found
empty definition using fallback
non-local guesses:

offset: 398
uri: file://<WORKSPACE>/spark/spark_ml_comparison.scala
text:
```scala
// spark_ml_train.scala
// IMPORTANT: Start Spark Shell with XGBoost:
// spark-shell --master local[*] --packages ml.dmlc:xgboost4j-spark_2.12:2.0.3
// scala> :paste  // paste code + Ctrl+D

import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}
import org.@@apache.spark.ml.classification._
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.sql.Row
import ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier
import org.apache.spark.ml.linalg.Vectors
import scala.collection.mutable.ListBuffer

// ====== CONFIG ‚Äì PATHS ======
val featuresPath = "hdfs://localhost:9000/user/hadoop/image_analysis/spark_features"
// =======================================

try {
  println(s"üì• Loading preprocessed features from: $featuresPath")

  val df = spark.read.parquet(featuresPath)

  println("üìã Input schema:")
  df.printSchema()

  println(s"üî¢ Total records: ${df.count()}")

  println("üëÄ Sample rows:")
  df.show(5, truncate = false)

  // ========= FIX Labels (all were 1; reassign based on keyword) =========
  val fixedDf = df.withColumn(
    "label",
    when(lower($"keyword") === "cat", 0.0).otherwise(1.0)
  )

  println("üìä Fixed Class distribution (label: 0=cat, 1=dog):")
  fixedDf.groupBy("label").count().show()

  // ========= 1. Select feature columns (MATCH YOUR SCHEMA) =========
  val featureCols = Array(
    "tag_count", "tag_text_len", "has_animals", "has_people", "has_city",
    "r_mean", "g_mean", "b_mean", "r_std", "g_std", "b_std",
    "gray_mean", "gray_std", "laplacian_var", "edge_density",
    "hog_0", "hog_1", "hog_2", "hog_3", "hog_4", "hog_5", "hog_6", "hog_7", "hog_8"
  )

  // Filter nulls
  val clean = fixedDf.na.drop("any", featureCols :+ "label")

  println(s"‚úÖ Rows before cleaning: ${fixedDf.count()}")
  println(s"‚úÖ Rows after cleaning:  ${clean.count()}")

  if (clean.count() < 10) {
    throw new Exception("Not enough data after cleaning! Check your preprocessing.")
  }

  // ========= 2. Train-test split =========
  val Array(train, test) = clean.randomSplit(Array(0.8, 0.2), seed = 42L)
  println(s"üìö Train size: ${train.count()}, Test size: ${test.count()}")

  // ========= 3. Vector Assembler (shared) =========
  val assembler = new VectorAssembler()
    .setInputCols(featureCols)
    .setOutputCol("features")
    .setHandleInvalid("skip")

  // ========= 4. Define evaluators =========
  val aucEval = new BinaryClassificationEvaluator()
    .setLabelCol("label")
    .setRawPredictionCol("rawPrediction")
    .setMetricName("areaUnderROC")

  val accEval = new MulticlassClassificationEvaluator()
    .setLabelCol("label")
    .setPredictionCol("prediction")
    .setMetricName("accuracy")

  // ========= 5. Train and evaluate function =========
  val results = ListBuffer[(String, Double, Double)]()  // (modelName, AUC, Accuracy)

  def trainAndEval(model: Classifier, modelName: String): Unit = {
    println(s"üöÄ Training $modelName...")

    val pipeline = new Pipeline().setStages(Array(assembler, model))
    val fittedModel = pipeline.fit(train)

    val predictions = fittedModel.transform(test)

    println(s"üîé $modelName predictions (sample):")
    predictions.select("label", "prediction", "probability").show(5, false)

    val auc = aucEval.evaluate(predictions)
    val acc = accEval.evaluate(predictions)

    results += ((modelName, auc, acc))

    println(f"üìà $modelName - Test AUC: $auc%1.4f, Accuracy: $acc%1.4f")

    // Confusion matrix
    println(s"üßÆ $modelName Confusion Matrix:")
    predictions.groupBy("label", "prediction").count().show()
  }

  // Logistic Regression
  val lr = new LogisticRegression()
    .setMaxIter(100)
    .setRegParam(0.01)
  trainAndEval(lr, "Logistic Regression")

  // Random Forest
  val rf = new RandomForestClassifier()
    .setNumTrees(50)
    .setMaxDepth(5)
  trainAndEval(rf, "Random Forest")

  // Decision Tree
  val dt = new DecisionTreeClassifier()
    .setMaxDepth(5)
  trainAndEval(dt, "Decision Tree")

  // Support Vector Machine (Linear SVM)
  val svm = new LinearSVC()
    .setMaxIter(100)
    .setRegParam(0.01)
  trainAndEval(svm, "Support Vector Machine")

  // Naive Bayes
  val nb = new NaiveBayes()
    .setSmoothing(1.0)
  trainAndEval(nb, "Naive Bayes")

  // XGBoost
  val xgb = new XGBoostClassifier(
    Map(
      "num_round" -> 50,
      "eta" -> 0.1,
      "max_depth" -> 5,
      "objective" -> "binary:logistic"
    )
  )
  trainAndEval(xgb, "XGBoost")

  // Neural Network (Multilayer Perceptron)
  val layers = Array[Int](featureCols.length, 64, 32, 2)  // input, hidden1, hidden2, output
  val nn = new MultilayerPerceptronClassifier()
    .setLayers(layers)
    .setBlockSize(128)
    .setSeed(42L)
    .setMaxIter(100)
  trainAndEval(nn, "Neural Network")

  // K-Nearest Neighbors (Exact, broadcast train since small data)
  println("üöÄ Training K-Nearest Neighbors (Exact, K=5)...")

  val trainBroadcast = sc.broadcast(train.select("features", "label").collect())

  val knnPreds = test.rdd.map { row =>
    val testFeat = row.getAs[org.apache.spark.ml.linalg.Vector]("features")
    val neighbors = trainBroadcast.value.map { trow =>
      val dist = Vectors.sqdist(testFeat, trow.getAs[org.apache.spark.ml.linalg.Vector](0))
      (dist, trow.getDouble(1))
    }.sortBy(_._1).take(5)  // Top 5 nearest

    val pred = neighbors.groupBy(_._2).mapValues(_.size).maxBy(_._2)._1  // Majority vote

    Row.fromSeq(row.toSeq :+ pred :+ Vectors.dense(1.0 - pred, pred))  // Add prediction, rawPrediction
  }

  val knnSchema = test.schema
    .add("prediction", "double")
    .add("rawPrediction", "vector")

  val knnDF = spark.createDataFrame(knnPreds, knnSchema)

  val knnAuc = aucEval.evaluate(knnDF)
  val knnAcc = accEval.evaluate(knnDF)
  results += (("K-Nearest Neighbors", knnAuc, knnAcc))

  println(f"üìà KNN - Test AUC: $knnAuc%1.4f, Accuracy: $knnAcc%1.4f")

  println("üßÆ KNN Confusion Matrix:")
  knnDF.groupBy("label", "prediction").count().show()

  // ========= 6. Compare all models =========
  println("\nüìä Model Comparison Table:")
  val resultsDF = spark.createDataFrame(results).toDF("Model", "AUC", "Accuracy")
  resultsDF.orderBy(desc("AUC")).show(false)

  // ========= 7. Save best model =========
  val bestModelName = resultsDF.orderBy(desc("AUC")).select("Model").head().getString(0)
  println(s"üèÜ Best model: $bestModelName")

  println("‚úÖ ML training and comparison finished.")

} catch {
  case e: Exception => println(s"‚ùå Error: ${e.getMessage}\n${e.getStackTrace.mkString("\n")}")
}
```


#### Short summary: 

empty definition using pc, found symbol in pc: 