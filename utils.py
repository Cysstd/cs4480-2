# utils.py - ä¸“æ³¨äºsrcå±æ€§çš„æ”¹è¿›ç‰ˆæœ¬

import re
import random
from urllib.parse import urlparse, urljoin
import requests
from PIL import Image
import numpy as np
from io import BytesIO
from datetime import datetime

def is_valid_image_url(url):
    """æ”¹è¿›çš„å›¾ç‰‡URLæ£€æµ‹ - ä¸“æ³¨äºsrcå±æ€§"""
    if not url or url.strip() == '':
        return False
    
    # é¦–å…ˆæ£€æŸ¥å¸¸è§çš„å›¾ç‰‡æ–‡ä»¶æ‰©å±•å
    valid_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']
    parsed_url = urlparse(url)
    path_lower = parsed_url.path.lower()
    
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    if any(path_lower.endswith(ext) for ext in valid_extensions):
        return True
    
    # æ£€æŸ¥å¸¸è§å›¾ç‰‡åŸŸåå’Œè·¯å¾„
    image_domains = ['wikimedia', 'upload.wikimedia', 'staticflickr', 'images.pexels']
    image_paths = ['/wiki/Special:FilePath/', '/thumb/', '/static/', '/images/']
    
    domain_valid = any(domain in parsed_url.netloc.lower() for domain in image_domains)
    path_valid = any(path in parsed_url.path for path in image_paths)
    
    return domain_valid or path_valid

def extract_image_urls_from_src(soup, base_url):
    """ä¸“é—¨ä»srcå±æ€§æå–å›¾ç‰‡URL"""
    img_urls = []
    
    print(f"ğŸ” Searching for img tags with src attributes...")
    
    # æŸ¥æ‰¾æ‰€æœ‰imgæ ‡ç­¾çš„srcå±æ€§
    img_tags = soup.find_all('img')
    print(f"ğŸ“· Found {len(img_tags)} img tags total")
    
    for i, img in enumerate(img_tags):
        src = img.get('src')
        if src:
            # æ„å»ºå®Œæ•´URL
            full_url = urljoin(base_url, src)
            
            if is_valid_image_url(full_url):
                img_urls.append(full_url)
                print(f"  âœ… Valid image URL: {full_url[:80]}...")
            else:
                print(f"  âŒ Invalid image URL (skipped): {full_url[:80]}...")
        else:
            # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„å±æ€§ï¼Œä½†ä¸»è¦å…³æ³¨src
            for attr in ['data-src', 'data-lazy-src']:
                alt_src = img.get(attr)
                if alt_src:
                    full_url = urljoin(base_url, alt_src)
                    if is_valid_image_url(full_url):
                        img_urls.append(full_url)
                        print(f"  âœ… Valid image URL from {attr}: {full_url[:80]}...")
    
    # å»é‡
    unique_urls = list(set(img_urls))
    print(f"ğŸ“Š Unique valid image URLs found: {len(unique_urls)}")
    
    return unique_urls

def assign_label(url):
    """åŸºäºURLæ¨¡å¼åˆ†é…æ ‡ç­¾"""
    url_lower = url.lower()
    
    cat_patterns = ['cat', 'kitten', 'feline', 'kitty']
    dog_patterns = ['dog', 'puppy', 'canine', 'pup']
    
    for pattern in cat_patterns:
        if pattern in url_lower:
            return 0  # çŒ«
    
    for pattern in dog_patterns:
        if pattern in url_lower:
            return 1  # ç‹—
    
    # å¦‚æœæ²¡æœ‰æ˜ç¡®æ¨¡å¼ï¼Œéšæœºåˆ†é…
    return random.randint(0, 1)

def clean_text_description(description, stop_words=None):
    """æ¸…æ´—æ–‡æœ¬æè¿°"""
    if description is None:
        return ""
        
    if stop_words is None:
        stop_words = set(['the', 'a', 'an', 'in', 'on', 'at', 'and', 'or', 'but'])
    
    clean_desc = re.sub(r'[^\w\s]', '', str(description).lower())
    words = clean_desc.split()
    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
    return ' '.join(filtered_words)

def download_image(img_url, session, timeout=8):
    """ä¸‹è½½å•å¼ å›¾ç‰‡ - ä¸“æ³¨äºsrc URL"""
    try:
        print(f"    ğŸ“¥ Attempting download: {img_url[:60]}...")
        
        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
        if not img_url.startswith(('http://', 'https://')):
            print(f"    âŒ Invalid URL scheme: {img_url}")
            return None
            
        img_response = session.get(img_url, timeout=timeout, stream=True)
        
        if img_response.status_code == 200:
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = img_response.headers.get('content-type', '')
            if 'image' not in content_type:
                print(f"    âŒ Not an image (content-type: {content_type})")
                return None
                
            # å°è¯•æ‰“å¼€å›¾ç‰‡
            try:
                image = Image.open(BytesIO(img_response.content))
                
                # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦æœ‰æ•ˆ
                if image.size[0] > 10 and image.size[1] > 10:
                    print(f"    âœ… Successfully downloaded image: {image.size}")
                    return image
                else:
                    print(f"    âŒ Image too small: {image.size}")
                    return None
                    
            except Exception as img_error:
                print(f"    âŒ Cannot open as image: {img_error}")
                return None
        else:
            print(f"    âŒ HTTP {img_response.status_code}")
            return None
            
    except Exception as e:
        print(f"    âŒ Download failed: {str(e)[:50]}")
        return None

def resize_image(image, size=(64, 64)):
    """è°ƒæ•´å›¾ç‰‡å¤§å°"""
    try:
        return image.resize(size, Image.Resampling.LANCZOS)
    except Exception as e:
        print(f"âŒ Image resize failed: {e}")
        return image

def create_session():
    """åˆ›å»ºè¯·æ±‚ä¼šè¯"""
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
    })
    return session