# python_scraper.py - ä¿®å¤ç‰ˆæœ¬

import time
import os
from urllib.parse import urlparse
import numpy as np
import json
import subprocess
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils import (
    download_image, resize_image, create_session, 
    validate_image_content, search_wikimedia_images, assign_label
)
from config import SEARCH_KEYWORDS, IMAGE_SIZE, MAX_IMAGES_PER_KEYWORD, PIXABAY_CONFIG

class PythonImageScraper:
    def __init__(self, save_original_images=True, save_dir=None):
        self.all_images_data = []
        self.all_images = []
        self.save_original_images = save_original_images
        
        if save_dir is None:
            self.save_dir = os.path.join(os.getcwd(), "pixabay_images")
        else:
            self.save_dir = save_dir
        
        if self.save_original_images:
            try:
                os.makedirs(self.save_dir, exist_ok=True)
                print(f"ğŸ“ å›¾ç‰‡å°†ä¿å­˜åˆ°: {self.save_dir}")
            except Exception as e:
                print(f"âŒ æ— æ³•åˆ›å»ºç›®å½•: {e}")
                self.save_original_images = False
    
    def scrape_and_save_to_hdfs(self, keywords=None):
        if keywords is None:
            keywords = SEARCH_KEYWORDS
        
        print("ğŸš€ Pythonçˆ¬è™«å¼€å§‹å·¥ä½œ...")
        start_time = time.time()
        
        for keyword in keywords:
            self._scrape_keyword(keyword)
        
        # å¦‚æœæ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®ï¼Œç›´æ¥å¤±è´¥
        if len(self.all_images_data) == 0:
            print("âŒ é”™è¯¯ï¼šæ²¡æœ‰è·å–åˆ°ä»»ä½•å›¾ç‰‡æ•°æ®ï¼")
            return None, 0
        
        metadata_file = self._save_metadata()
        self._upload_to_hdfs(metadata_file)
        
        scraping_time = time.time() - start_time
        print(f"âœ… Pythonçˆ¬è™«å®Œæˆï¼Œè€—æ—¶: {scraping_time:.2f}ç§’")
        
        return self.all_images_data, scraping_time
    
    def _scrape_keyword(self, keyword):
        print(f"ğŸ¯ çˆ¬å–å…³é”®è¯: {keyword}")
        
        local_session = create_session()
        
        # è·å–å›¾ç‰‡æ•°æ®ï¼ˆåŒ…å«æ ‡ç­¾ä¿¡æ¯ï¼‰
        images_data = search_wikimedia_images(keyword, MAX_IMAGES_PER_KEYWORD)
        
        if not images_data:
            print(f"âŒ æ— æ³•è·å– {keyword} å›¾ç‰‡æ•°æ®")
            return
        
        successful_downloads = 0
        for i, img_data in enumerate(images_data):
            if successful_downloads >= MAX_IMAGES_PER_KEYWORD:
                break
                
            try:
                img_url = img_data.get('image_url')
                if not img_url:
                    continue
                    
                image = download_image(img_url, local_session)
                if image and validate_image_content(np.array(image)):
                    image_resized = resize_image(image, IMAGE_SIZE)
                    image_array_resized = np.array(image_resized)
                    
                    if validate_image_content(image_array_resized):
                        # ä½¿ç”¨åŸºäºtagsçš„æ ‡ç­¾ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨æ—§çš„assign_label
                        label = img_data.get('label')
                        if label == -1:  # å¦‚æœæ— æ³•åŸºäºtagsç¡®å®šæ ‡ç­¾
                            label = assign_label(img_url, keyword)
                        
                        image_data = {
                            'image_id': f"{keyword}_{img_data.get('api_id', i)}",
                            'image_array': image_array_resized.tolist(),
                            'source_url': img_url,
                            'description': f"Pixabay {keyword} image",
                            'download_timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                            'label': label,
                            'keyword': keyword,
                            'original_shape': list(np.array(image).shape),
                            'tags': img_data.get('tags', ''),
                            'views': img_data.get('views', 0),
                            'downloads': img_data.get('downloads', 0),
                            'user': img_data.get('user', '')
                        }
                        
                        self.all_images_data.append(image_data)
                        self.all_images.append(image_array_resized)
                        successful_downloads += 1
                        print(f"    âœ… {keyword} å›¾ç‰‡ {successful_downloads} çˆ¬å–æˆåŠŸ (æ ‡ç­¾: {label})")
                        
            except Exception as e:
                print(f"    âŒ å¤„ç†å¤±è´¥: {e}")
                continue
        
        if successful_downloads == 0:
            print(f"âŒ {keyword} æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•å›¾ç‰‡")
        else:
            print(f"âœ… {keyword} æˆåŠŸä¸‹è½½ {successful_downloads} å¼ å›¾ç‰‡")
    
    def _save_metadata(self):
        metadata_file = "image_metadata.json"
        try:
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_images_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ å…ƒæ•°æ®ä¿å­˜åˆ°: {metadata_file}")
            return metadata_file
        except Exception as e:
            print(f"âŒ ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
            return None
    
    def _upload_to_hdfs(self, metadata_file):
        try:
            # åˆ›å»ºHDFSç›®å½•
            subprocess.run(["hdfs", "dfs", "-mkdir", "-p", "/user/hadoop/image_analysis"], 
                         check=True, capture_output=True)
            
            if metadata_file and os.path.exists(metadata_file):
                subprocess.run(["hdfs", "dfs", "-put", "-f", metadata_file, 
                              "/user/hadoop/image_analysis/"], 
                             check=True, capture_output=True)
                print("âœ… å…ƒæ•°æ®ä¸Šä¼ åˆ°HDFS: /user/hadoop/image_analysis/image_metadata.json")
            
            if self.save_original_images and os.path.exists(self.save_dir):
                subprocess.run(["hdfs", "dfs", "-put", "-f", self.save_dir, 
                              "/user/hadoop/image_analysis/"], 
                             check=True, capture_output=True)
                print("âœ… å›¾ç‰‡æ•°æ®ä¸Šä¼ åˆ°HDFS")
                
        except subprocess.CalledProcessError as e:
            print(f"âŒ HDFSä¸Šä¼ å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿HadoopæœåŠ¡å·²å¯åŠ¨: start-dfs.sh && start-yarn.sh")
        except Exception as e:
            print(f"âŒ HDFSæ“ä½œé”™è¯¯: {e}")

if __name__ == "__main__":
    scraper = PythonImageScraper(save_original_images=True)
    images_data, time_taken = scraper.scrape_and_save_to_hdfs()
    
    if images_data:
        print(f"ğŸ¯ æ€»å…±è·å– {len(images_data)} å¼ å›¾ç‰‡")
        
        # ç®€å•ç»Ÿè®¡
        cat_count = sum(1 for img in images_data if img['label'] == 0)
        dog_count = sum(1 for img in images_data if img['label'] == 1)
        print(f"ğŸ“Š åˆ†ç±»ç»Ÿè®¡: çŒ«({cat_count}), ç‹—({dog_count})")
    else:
        print("âŒ çˆ¬è™«å¤±è´¥ï¼šæ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")