# parallel_scraper.py - ä¸“æ³¨äºsrcå±æ€§çš„ç‰ˆæœ¬
# 123123213
import time
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import multiprocessing as mp
from multiprocessing import Pool, cpu_count
import numpy as np

from utils import extract_image_urls_from_src, assign_label, download_image, resize_image, create_session
from config import WEBSITES_TO_SCRAPE, IMAGE_SIZE, MAX_IMAGES_PER_SITE

class ParallelWebScraper:
    def __init__(self):
        self.num_cores = cpu_count()
        self.all_images_data = []
        self.all_images = []
    
    def scrape_single_website(self, url):
        """çˆ¬å–å•ä¸ªç½‘ç«™ - ä¸“æ³¨äºsrcå±æ€§"""
        website_images_data = []
        website_images = []
        local_session = create_session()
        
        try:
            print(f"ğŸ” Scraping: {url}")
            response = local_session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä½¿ç”¨ä¸“é—¨ä»srcå±æ€§æå–URLçš„å‡½æ•°
            image_urls = extract_image_urls_from_src(soup, url)
            
            print(f"ğŸ–¼ï¸ Found {len(image_urls)} valid image URLs at {url}")
            
            # ä¸‹è½½å’Œå¤„ç†å›¾ç‰‡
            successful_downloads = 0
            for i, img_url in enumerate(image_urls[:MAX_IMAGES_PER_SITE]):
                try:
                    print(f"  [{i+1}/{min(len(image_urls), MAX_IMAGES_PER_SITE)}] Processing: {img_url[:70]}...")
                    
                    image = download_image(img_url, local_session)
                    if image is not None:
                        # è½¬æ¢ä¸ºRGBï¼ˆå¤„ç†PNGé€æ˜èƒŒæ™¯ç­‰é—®é¢˜ï¼‰
                        if image.mode != 'RGB':
                            image = image.convert('RGB')
                            
                        image_array = np.array(image)
                        
                        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸å’Œè´¨é‡
                        if (len(image_array.shape) == 3 and  # å¿…é¡»æ˜¯å½©è‰²å›¾ç‰‡
                            image_array.shape[0] >= 50 and 
                            image_array.shape[1] >= 50 and
                            image_array.shape[2] == 3):  # å¿…é¡»æœ‰3ä¸ªé¢œè‰²é€šé“
                            
                            image_resized = resize_image(image, IMAGE_SIZE)
                            image_array_resized = np.array(image_resized)
                            
                            label = assign_label(img_url)
                            
                            image_data = {
                                'image_id': f"scraped_{mp.current_process().pid}_{i}",
                                'image_array': image_array_resized,
                                'source_url': img_url,
                                'description': f"Scraped from {urlparse(url).netloc}",
                                'download_timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                                'label': label,
                                'original_shape': image_array.shape
                            }
                            
                            website_images_data.append(image_data)
                            website_images.append(image_array_resized)
                            successful_downloads += 1
                            print(f"  âœ… Successfully stored image {successful_downloads}")
                        else:
                            print(f"  âš ï¸  Image rejected: invalid shape {image_array.shape}")
                    else:
                        print(f"  âŒ Failed to process image")
                        
                except Exception as e:
                    print(f"  âŒ Error processing image {i+1}: {str(e)[:80]}")
                    continue
            
            print(f"âœ… Downloaded {successful_downloads} images from {url}")
            
        except Exception as e:
            print(f"âŒ Error scraping {url}: {e}")
        
        finally:
            local_session.close()
        
        return website_images_data, website_images
    
    def run_parallel_scraping(self, websites=None):
        """è¿è¡Œå¹¶è¡Œç½‘é¡µçˆ¬å–"""
        if websites is None:
            websites = WEBSITES_TO_SCRAPE
        
        print("ğŸš€ Starting parallel web scraping...")
        print(f"ğŸ–¥ï¸  CPU cores available: {self.num_cores}")
        print(f"ğŸ¯ Target websites: {len(websites)}")
        
        start_time = time.time()
        
        # ä½¿ç”¨è¿›ç¨‹æ± è¿›è¡Œå¹¶è¡Œçˆ¬å–
        with Pool(processes=min(self.num_cores, len(websites))) as pool:
            results = pool.map(self.scrape_single_website, websites)
        
        # åˆå¹¶ç»“æœ
        for website_images_data, website_images in results:
            if website_images_data:
                self.all_images_data.extend(website_images_data)
                self.all_images.extend(website_images)
        
        scraping_time = time.time() - start_time
        print(f"âš¡ Parallel scraping completed in {scraping_time:.2f} seconds")
        print(f"ğŸ“Š Total images scraped: {len(self.all_images_data)}")
        
        return self.all_images_data, self.all_images, scraping_time

    def create_test_data(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆå¦‚æœçˆ¬å–å¤±è´¥ï¼‰"""
        print("ğŸ› ï¸ Creating test data for development...")
        
        # åˆ›å»ºä¸€äº›ç®€å•çš„æµ‹è¯•å›¾ç‰‡
        test_images_data = []
        test_images = []
        
        for i in range(12):  # åˆ›å»º12å¼ æµ‹è¯•å›¾ç‰‡
            # åˆ›å»ºéšæœºå›¾ç‰‡
            if i % 2 == 0:
                # åˆ›å»º"çŒ«"å›¾ç‰‡ï¼ˆåæ©™è‰²ï¼‰
                img_array = np.random.randint(200, 255, (64, 64, 3), dtype=np.uint8)
                img_array[:, :, 0] = np.random.randint(200, 255, (64, 64))  # æ›´å¤šçº¢è‰²
                img_array[:, :, 2] = np.random.randint(0, 100, (64, 64))    # è¾ƒå°‘è“è‰²
                label = 0
                desc = "Test cat image from src scraping"
            else:
                # åˆ›å»º"ç‹—"å›¾ç‰‡ï¼ˆåæ£•è‰²ï¼‰
                img_array = np.random.randint(150, 200, (64, 64, 3), dtype=np.uint8)
                img_array[:, :, 0] = np.random.randint(150, 200, (64, 64))  # ä¸­ç­‰çº¢è‰²
                img_array[:, :, 1] = np.random.randint(100, 150, (64, 64))  # ä¸­ç­‰ç»¿è‰²
                label = 1
                desc = "Test dog image from src scraping"
            
            image_data = {
                'image_id': f"test_{i}",
                'image_array': img_array,
                'source_url': f"https://example.com/test_{i}.jpg",
                'description': desc,
                'download_timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'label': label,
                'original_shape': img_array.shape
            }
            
            test_images_data.append(image_data)
            test_images.append(img_array)
        
        print(f"ğŸ“Š Created {len(test_images_data)} test images")
        return test_images_data, test_images, 0.1

if __name__ == "__main__":
    # å•ç‹¬æµ‹è¯•çˆ¬å–æ¨¡å—
    scraper = ParallelWebScraper()
    
    try:
        images_data, images, time_taken = scraper.run_parallel_scraping()
        if len(images_data) == 0:
            print("âš ï¸  No images scraped from src attributes, creating test data...")
            images_data, images, time_taken = scraper.create_test_data()
    except Exception as e:
        print(f"âŒ Scraping failed: {e}")
        print("ğŸ› ï¸ Creating test data instead...")
        images_data, images, time_taken = scraper.create_test_data()
    
    print(f"æµ‹è¯•å®Œæˆ: å¤„ç† {len(images_data)} å¼ å›¾ç‰‡, è€—æ—¶ {time_taken:.2f}ç§’")